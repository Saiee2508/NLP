{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeJXhUj448zDn+Lsi5bCVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saiee2508/NLP/blob/main/NLP_Assinment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxXKx6ZwskOP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, regexp_tokenize, TweetTokenizer, WhitespaceTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zn26hyEsxB2",
        "outputId": "8883d643-1828-4580-c006-8af8d9f1e691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Data\n"
      ],
      "metadata": {
        "id": "XWwzb89SIgyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Running is fun! Let's go for a run... excited\""
      ],
      "metadata": {
        "id": "23QZRoZNtZqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Whitespace Tokenizer**:A whitespace tokenizer is a simple text tokenization method where the input text is split into tokens (words or phrases) based on whitespace (spaces, tabs, newlines)"
      ],
      "metadata": {
        "id": "qHCwde61IlXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
        "print(\"Whitespace Tokens:\", whitespace_tokens)\n"
      ],
      "metadata": {
        "id": "V-5CmANCtoeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf0765a-0b6c-4df9-9aec-9168d3629cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokens: ['Running', 'is', 'fun!', \"Let's\", 'go', 'for', 'a', 'run...', 'excited']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**punctuation tokenizer**:A punctuation tokenizer splits text based on punctuation marks (such as periods, commas, exclamation marks, etc.)"
      ],
      "metadata": {
        "id": "kf2i44NyJOLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def punctuation_tokenizer(text):\n",
        "    return re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "\n",
        "text = \"Hello, world! How's it going?\"\n",
        "tokens = punctuation_tokenizer(text)\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "e9QrTl6EtqZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459d6df5-c06a-4ec8-c0c6-543aa7ed7925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'How', \"'\", 's', 'it', 'going', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "print(\"Treebank Tokens:\", treebank_tokens)\n"
      ],
      "metadata": {
        "id": "vRVnqB9dtsf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62af2af8-6db6-41b9-90f0-554504a163d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokens: ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'going', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"Tweet Tokens:\", tweet_tokens)"
      ],
      "metadata": {
        "id": "Jjx3wvJptu0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c1d49d-097b-4eaf-b2d8-a2bbbe9526ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokens: ['Hello', ',', 'world', '!', \"How's\", 'it', 'going', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stems = [porter_stemmer.stem(word) for word in treebank_tokens]\n",
        "snowball_stems = [snowball_stemmer.stem(word) for word in treebank_tokens]\n",
        "print(\"Snowball Stemmed Tokens:\", snowball_stems)\n"
      ],
      "metadata": {
        "id": "iZn0RG0pt02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad955cf-45ee-4b72-ef24-a4214e767a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmed Tokens: ['hello', ',', 'world', '!', 'how', \"'s\", 'it', 'go', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "U475mglht3qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in treebank_tokens]\n",
        "print(\"Lemmatized Tokens (verbs):\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "dZnd5kqPt7UN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd17c22-c229-4689-9d1d-bd48e6f82294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens (verbs): ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'go', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Whitespace Tokens:\", whitespace_tokens)\n",
        "print(\"Treebank Tokens:\", treebank_tokens)\n",
        "print(\"Tweet Tokens:\", tweet_tokens)\n",
        "print(\"Porter Stemmed Tokens:\", porter_stems)\n",
        "print(\"Snowball Stemmed Tokens:\", snowball_stems)\n",
        "print(\"Lemmatized Tokens (verbs):\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xbMQFLxt-bt",
        "outputId": "b1676757-4aea-433a-b257-8335547a1056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokens: ['Running', 'is', 'fun!', \"Let's\", 'go', 'for', 'a', 'run...', 'excited']\n",
            "Treebank Tokens: ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'going', '?']\n",
            "Tweet Tokens: ['Hello', ',', 'world', '!', \"How's\", 'it', 'going', '?']\n",
            "Porter Stemmed Tokens: ['hello', ',', 'world', '!', 'how', \"'s\", 'it', 'go', '?']\n",
            "Snowball Stemmed Tokens: ['hello', ',', 'world', '!', 'how', \"'s\", 'it', 'go', '?']\n",
            "Lemmatized Tokens (verbs): ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'go', '?']\n"
          ]
        }
      ]
    }
  ]
}